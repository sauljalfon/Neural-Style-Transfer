{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H-4qmuIgDaYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6031a86-bac8-49e4-daf0-35526bb54238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2267729893.py:4: DeprecationWarning: scipy.misc is deprecated and will be removed in 2.0.0\n",
            "  import scipy.misc\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer Learning"
      ],
      "metadata": {
        "id": "J33yXOjzN7AW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(272)\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "img_size = 400\n",
        "vgg = tf.keras.applications.VGG19(include_top=False,\n",
        "                                  input_shape=(img_size, img_size, 3),\n",
        "                                  weights='pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "vgg.trainable = False\n",
        "pp.pprint(vgg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "2Bx_0eGLN9iK",
        "outputId": "e03a75b3-0c89-42ac-94f7-1c8bc8ec46e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The `weights` argument should be either `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: weights=pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3665858586.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrettyPrinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m vgg = tf.keras.applications.VGG19(include_top=False,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                   \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                   weights='pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/applications/vgg19.py\u001b[0m in \u001b[0;36mVGG19\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, name)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \"\"\"\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"imagenet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfile_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;34m\"The `weights` argument should be either \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;34m\"`None` (random initialization), 'imagenet' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The `weights` argument should be either `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: weights=pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Style Transfer"
      ],
      "metadata": {
        "id": "MQ_DtRBtEFNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = Image.open(\"images/louvre.jpg\")\n",
        "content_image"
      ],
      "metadata": {
        "id": "7ighcWZlFTin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Cost"
      ],
      "metadata": {
        "id": "M6CAtYfvSuPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_content_cost(content_output, generated_output):\n",
        "  \"\"\"\n",
        "  Computes the cost function\n",
        "\n",
        "  Argumets:\n",
        "   a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations\n",
        "    representing content of the image C\n",
        "\n",
        "   a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations\n",
        "    representing content of the image G\n",
        "\n",
        "  Returns:\n",
        "    J_content -- scalar that you compute using equation 1 above.\n",
        "  \"\"\"\n",
        "  a_C = content_output[-1]\n",
        "  a_G = generated_output[-1]\n",
        "\n",
        "  # Retrieve dimensions from a_G\n",
        "  _, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
        "  print(n_H)\n",
        "\n",
        "  # Reshape 'a_C' and 'a_G'\n",
        "  a_C_unrolled = tf.reshape(a_C, shape=[-1])\n",
        "  a_G_unrolled = tf.reshape(a_G, shape=[-1])\n",
        "\n",
        "  # Compute the Cost\n",
        "  J_content = tf.reduce_sum(tf.square(a_C_unrolled - a_G_unrolled)) / (4 * n_H * n_W * n_C)\n",
        "\n",
        "  return J_content"
      ],
      "metadata": {
        "id": "pMD5pdVpGubw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute the Style Cost"
      ],
      "metadata": {
        "id": "CYQ2OiScSzbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = Image.open(\"images/monet_800600.jpg\")\n",
        "example"
      ],
      "metadata": {
        "id": "pUKcP_UAQ20L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(A):\n",
        "  \"\"\"\n",
        "    Argument:\n",
        "    A -- matrix of shape (n_C, n_H*n_W)\n",
        "\n",
        "    Returns:\n",
        "    GA -- Gram matrix of A, of shape (n_C, n_C)\n",
        "  \"\"\"\n",
        "  GA = tf.linalg.matmul(A, tf.transpose(A))\n",
        "  return GA"
      ],
      "metadata": {
        "id": "LSOQbhjoS30z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_layer_style_cost(a_S, a_G):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S\n",
        "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n",
        "\n",
        "    Returns:\n",
        "    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)\n",
        "    \"\"\"\n",
        "    # Retrieve dimensions from a_G\n",
        "    _, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
        "\n",
        "    # Reshape the tensors from (1, n_H, n_W, n_C) to (n_C, n_H * n_W)\n",
        "    a_S = tf.transpose(tf.reshape(a_S, shape = [-1, n_C]))\n",
        "    a_G = tf.transpose(tf.reshape(a_G, shape = [-1, n_C]))\n",
        "\n",
        "    # Computing gram_matrices for both images S and G\n",
        "    GS = gram_matrix(a_S)\n",
        "    GG = gram_matrix(a_G)\n",
        "\n",
        "    # Computing the loss\n",
        "    J_style_layer = tf.reduce_sum(tf.square(GS - GG)) / (4.0 * (n_C ** 2) * ((n_H * n_W) ** 2))\n",
        "\n",
        "    return J_style_layer"
      ],
      "metadata": {
        "id": "de9X-VpXTO-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in vgg.layers:\n",
        "    print(layer.name)"
      ],
      "metadata": {
        "id": "LWaSnjNhUvps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg.get_layer('block5_conv4').output"
      ],
      "metadata": {
        "id": "_9ky-Kj3VAni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STYLE_LAYERS = [\n",
        "    ('block1_conv1', 0.2),\n",
        "    ('block2_conv1', 0.2),\n",
        "    ('block3_conv1', 0.2),\n",
        "    ('block4_conv1', 0.2),\n",
        "    ('block5_conv1', 0.2)]"
      ],
      "metadata": {
        "id": "3VSWZlpMVBDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_style_cost(style_image_output, generated_image_output, STYLE_LAYERS=STYLE_LAYERS):\n",
        "    \"\"\"\n",
        "    Computes the overall style cost from several chosen layers\n",
        "\n",
        "    Arguments:\n",
        "    style_image_output -- our tensorflow model\n",
        "    generated_image_output --\n",
        "    STYLE_LAYERS -- A python list containing:\n",
        "                        - the names of the layers we would like to extract style from\n",
        "                        - a coefficient for each of them\n",
        "\n",
        "    Returns:\n",
        "    J_style -- tensor representing a scalar value, style cost defined above by equation (2)\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize the overall style cost\n",
        "    J_style = 0\n",
        "\n",
        "    # Set a_S to be the hidden layer activation from the layer we have selected.\n",
        "    a_S = style_image_output[:-1]\n",
        "\n",
        "    # Set a_G to be the output of the choosen hidden layers.\n",
        "    a_G = generated_image_output[:-1]\n",
        "    for i, weight in zip(range(len(a_S)), STYLE_LAYERS):\n",
        "        # Compute style_cost for the current layer\n",
        "        J_style_layer = compute_layer_style_cost(a_S[i], a_G[i])\n",
        "\n",
        "        # Add weight * J_style_layer of this layer to overall style cost\n",
        "        J_style += weight[1] * J_style_layer\n",
        "\n",
        "    return J_style"
      ],
      "metadata": {
        "id": "mEB5LSQKVOe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function()\n",
        "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
        "    \"\"\"\n",
        "    Computes the total cost function\n",
        "\n",
        "    Arguments:\n",
        "    J_content -- content cost coded above\n",
        "    J_style -- style cost coded above\n",
        "    alpha -- hyperparameter weighting the importance of the content cost\n",
        "    beta -- hyperparameter weighting the importance of the style cost\n",
        "\n",
        "    Returns:\n",
        "    J -- total cost as defined by the formula above.\n",
        "    \"\"\"\n",
        "    J = alpha * J_content + beta * J_style\n",
        "\n",
        "    return J"
      ],
      "metadata": {
        "id": "ZsdBMfIsVdXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization Problem"
      ],
      "metadata": {
        "id": "RQuj0HwuVnaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the content image\n",
        "2. Load the style image\n",
        "3. Randomly initialize the image to be generated\n",
        "4. Load the VGG19 model\n",
        "5. Compute the content cost\n",
        "6. Compute the style cost\n",
        "7. Compute the total cost\n",
        "8. Define the optimizer and learning rate\n"
      ],
      "metadata": {
        "id": "iuwQg6rjVu8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Content Image"
      ],
      "metadata": {
        "id": "ynSlS3f3WAiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = np.array(Image.open(\"images/cat.jpg\").resize((img_size, img_size)))\n",
        "content_image = tf.constant(np.reshape(content_image, ((1,) + content_image.shape)))\n",
        "\n",
        "print(content_image.shape)\n",
        "imshow(content_image[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zULQ-k6yV1hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Style Image"
      ],
      "metadata": {
        "id": "WKBqHoBhWEXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "style_image =  np.array(Image.open(\"images/stone_style.jpg\").resize((img_size, img_size)))\n",
        "style_image = tf.constant(np.reshape(style_image, ((1,) + style_image.shape)))\n",
        "\n",
        "print(style_image.shape)\n",
        "imshow(style_image[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UaaCrB0-V8mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Randomly the Image to be Generated"
      ],
      "metadata": {
        "id": "eNMaLeQ8WN4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
        "noise = tf.random.uniform(tf.shape(generated_image), -0.25, 0.25)\n",
        "generated_image = tf.add(generated_image, noise)\n",
        "generated_image = tf.clip_by_value(generated_image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "print(generated_image.shape)\n",
        "imshow(generated_image.numpy()[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QWb0a1yJWhpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pre-Trained VGG19 Model"
      ],
      "metadata": {
        "id": "NeOXC3zRWwyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_layer_outputs(vgg, layer_names):\n",
        "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
        "    outputs = [vgg.get_layer(layer[0]).output for layer in layer_names]\n",
        "\n",
        "    model = tf.keras.Model([vgg.input], outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "qj5ZIBINi5Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_layer = [('block5_conv4', 1)]\n",
        "vgg_model_outputs = get_layer_outputs(vgg, STYLE_LAYERS + content_layer)"
      ],
      "metadata": {
        "id": "g5APWaBZi7xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute the Total Cost"
      ],
      "metadata": {
        "id": "oQyAJxMii-jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_content =  tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
        "a_C = vgg_model_outputs(preprocessed_content)"
      ],
      "metadata": {
        "id": "zFMEGQghjEWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compute the style image encoding (a_S)"
      ],
      "metadata": {
        "id": "Z5EK4azfjGaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the input of the model to be the \"style\" image\n",
        "preprocessed_style =  tf.Variable(tf.image.convert_image_dtype(style_image, tf.float32))\n",
        "a_S = vgg_model_outputs(preprocessed_style)"
      ],
      "metadata": {
        "id": "i8aSRDQfjN4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_0_1(image):\n",
        "    \"\"\"\n",
        "    Truncate all the pixels in the tensor to be between 0 and 1\n",
        "\n",
        "    Arguments:\n",
        "    image -- Tensor\n",
        "    J_style -- style cost coded above\n",
        "\n",
        "    Returns:\n",
        "    Tensor\n",
        "    \"\"\"\n",
        "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "def tensor_to_image(tensor):\n",
        "    \"\"\"\n",
        "    Converts the given tensor into a PIL image\n",
        "\n",
        "    Arguments:\n",
        "    tensor -- Tensor\n",
        "\n",
        "    Returns:\n",
        "    Image: A PIL image\n",
        "    \"\"\"\n",
        "    tensor = tensor * 255\n",
        "    tensor = np.array(tensor, dtype=np.uint8)\n",
        "    if np.ndim(tensor) > 3:\n",
        "        assert tensor.shape[0] == 1\n",
        "        tensor = tensor[0]\n",
        "    return Image.fromarray(tensor)"
      ],
      "metadata": {
        "id": "lIpwkLBXjQQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Step"
      ],
      "metadata": {
        "id": "4Gs_kchxjVkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "@tf.function()\n",
        "def train_step(generated_image):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Compute a_G as the vgg_model_outputs for the current generated image\n",
        "        a_G = vgg_model_outputs(generated_image)\n",
        "\n",
        "        # Compute the style cost\n",
        "        J_style = compute_style_cost(a_S, a_G, STYLE_LAYERS)\n",
        "\n",
        "        # Compute the content cost\n",
        "        J_content = compute_content_cost(a_C, a_G)\n",
        "        # Compute the total cost\n",
        "        J = total_cost(J_content, J_style, alpha = 10, beta = 40)\n",
        "\n",
        "    grad = tape.gradient(J, generated_image)\n",
        "\n",
        "    optimizer.apply_gradients([(grad, generated_image)])\n",
        "    generated_image.assign(clip_0_1(generated_image))\n",
        "    return J"
      ],
      "metadata": {
        "id": "mOPDVYJNjZ8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_image = tf.Variable(generated_image)"
      ],
      "metadata": {
        "id": "xBtZOHG0jtsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train The Model"
      ],
      "metadata": {
        "id": "aK_4jWG3j4zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5000\n",
        "for i in range(epochs):\n",
        "    train_step(generated_image)\n",
        "    if i % 250 == 0:\n",
        "        print(f\"Epoch {i} \")\n",
        "    if i % 250 == 0:\n",
        "        image = tensor_to_image(generated_image)\n",
        "        imshow(image)\n",
        "        image.save(f\"output/image_{i}.jpg\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "nuMPDOGAkDD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dmYB3RWYkDaM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}